%        File: proposal.tex
%     Created: Mon Sep 26 12:00 AM 2016 M
% Last Change: Mon Sep 26 12:00 AM 2016 M
%
\documentclass[a4paper]{article}
\usepackage{listings}

\title{RSA Encryption and Decryption}
\author{Kevin Liao and Josh Smith}
\date{November 28, 2016}


\begin{document}

\maketitle

\section{Introduction}
The RSA cryptosystem is one of the most widely used public-key cryptosystems in use today for securing information. Fundamentally, it allows two parties to exchange a secret message who have never communicated in the past. To accomplish this, RSA utilizes a pair of keys, a public key for encryption and a private key for decryption. The encryption and decryption keys are distinct, and so RSA is often referred to as an asymmetric cryptosystem.

For this project, we studied the RSA cryptosystem to understand how and why it works. As one of the most mature cryptosystems, RSA has been studied extensively, and there are plenty of interesting resources on attacks and how to prevent them~\cite{boneh1999twenty}. These attacks provide an excellent exposition for the dangers of improperly implementing RSA, which makes such a project well-suited for learning.

We focused on the number theory behind the algorithm, well-known attacks on the RSA cryptosysem, and secure coding practices associated with implementing cryptosystems more broadly. We implemented the RSA encryption and decryption algorithms according to cryptographic considerations for security and performance and according the well-established specifications. This provided a better understanding of the nuances of cryptographic coding in practice.

\section{Implementation}

We first detail how we handle multiple precision numbers, then we detail our implementation of RSA key generation and encryption and decryption functions.

\subsection{Handling Multiple Precision Numbers}

Even before starting the implementation of PKCS $\#1$~\cite{rsa2012pkcs} itself, the first major challenge we faced was deciding how to store the numbers that would be used for encryption. Typical RSA integers are on the order of 1000 bits in size, which far exceeds the capacity of standard C data types. Thus, some custom BigInteger data type was necessary to store integers of arbitrary precision. Though less of a security concern, this was nonetheless a fundamental part of implementing the encryption scheme.

To gain experience working with arbitrary precision integers, we initially attempted to create the BigInteger library ourselves. Three primary design decisions guided the process. First of all, to make memory usage efficient, we used dynamically-sized integers. This allowed integers to occupy only the memory they required, and freed up any they didn't. It also had the additional benefit of placing no limit on the capacity of a BigInteger. Secondly, intending to replicate the behavior of primitive C data types, we did not use in-place operations on BigIntegers. That is, the output of any BigInteger operation was a newly allocated BigInteger, and the operands were unchanged. Finally, we decided not to represent negative integers. This is sufficient for RSA, and had the advantage of simplicity.

In the end, our custom solution was quite inefficient, and fixing all its issues would have likely required a complete redesign. Thus, we decided instead to incorporate a preexisting library to handle multiple-precision integers. For this purpose, we settled on GMP (the GNU Multi-Precision library)~\cite{gmpmultiple}.

\subsection{Key Pair Generation}

We follow the Digital Signature Standard (DSS)~\cite{fips2013186} issued by the National Institute of Standards and Technology (NIST) to generate key pairs.

\subsubsection{Pseudorandom Number Generator}\label{sec:prng}
In order to generate random primes, it is important that we use a cryptographically secure pseudorandom number generator. We decide to use the UNIX-based special file {\tt /dev/random}, which generates high-quality pseudorandom numbers that are well-suited for key generation.

The semantics for {\tt /dev/random} vary based on the operating system. In Linux,  {\tt /dev/random} is generated from entropy created by keystrokes, mouse movements, IDE timings, and other kernel processes. In macOS, {\tt /dev/random} data is generated using the Yarrow-160 algorithm, which is a cryptographic pseudorandom number generator. Yarrow-160 outputs random bits using a combination of the SHA1 hash function and three-key triple-DES.

We believe {\tt /dev/random}, as prescribed, is sufficient for our purposes, but the entropy pool can be further improved using specialized programs or hardware random number generators.

\subsubsection{Primality Testing}

We use the Miller-Rabin probabilistic primality test to validate the generation of prime numbers. There are two approaches for using Miller-Rabin primality testing: (1) using several iterations of Miller-Rabin alone; (2) using several iterations of Miller-Rabin followed by a Lucas primality test. For simplicity, we use the iterative Miller-Rabin implementation available in the GNU MP Library. Instead, we find it more interesting to learn how to use Miller-Rabin testing correctly
in practice, as specified in the DSS.

For example, different modulus lengths for RSA require varying rounds of Miller-Rabin testing. We reproduce the number of rounds necessary for various auxiliary prime (see Section~\ref{sec:keygen}) lengths in Table~\ref{tab:mr}, and we follow this in our implementation.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|} 
 \hline
 Auxiliary Prime Length & Rounds of M-R Testing  \\ \hline
 $>100$ bits & 28 \\ 
 $>140$ bits & 38 \\ 
 $>170$ bits & 41 \\
 \hline
\end{tabular}
\caption{The table shows the number of Miller-Rabin rounds necessary as a function of the lengths of auxiliary primes $p_1$, $p_2$, $q_1$, and $q_2$.}
\label{tab:mr}
\end{table}



\subsubsection{Criteria for Key Pairs}\label{sec:keygen}

The key pair for RSA consists of the public key $(n, e)$ and the private key $(n, d)$. The RSA modulus $n$ is the product of two distinct prime numbers $p$ and $q$. RSA's security rests on the primality and secrecy of $p$ and $q$, as well as the secrecy of the private exponent $d$. The methodology for generating these parameters varies based on the desired number of bits of security and the desired quality of primes. However, several desideratum must hold true for all
methods.\newline

\noindent \textbf{Public Exponent $e$.} The following constraints must hold true for the public exponent $e$.
\begin{enumerate}
    \item The public verification exponent $e$ must be selected prior to generating the primes $p$ and $q$, and the private signature exponent $d$.

    \item The public verification exponent $e$ must be an odd positive integer such that $2^{16} < e < 2^{256}$.

\end{enumerate}

It is immaterial whether or not $e$ is a fixed value or a random value, as long as it satisfies constraint 2 above. For simplicity, we fix $e = 2^{16} + 1 = 65537$.\newline

\noindent \textbf{Primes $p$ and $q$.} The following constraints must hold true for random primes $p$ and $q$.

\begin{enumerate}
    \item Both $p$ and $q$ shall be either provable primes or probable primes.

    \item Both $p$ and $q$ shall be randomly generated prime numbers such that all of the following subconstraints hold:

        \begin{itemize}
            \item $(p+1)$ has a prime factor $p_1$
            \item $(p-1)$ has a prime factor $p_2$
            \item $(q+1)$ has a prime factor $q_1$
            \item $(q-1)$ has a prime factor $q_2$
        \end{itemize}

    where $p_1$, $p_2$, $q_1$, $q_2$ are auxiliary primes of $p$ and $q$. Then, one of the following shall also apply:

    \begin{enumerate}
        \item[(i)] $p_1$, $p_2$, $q_1$, $q_2$, $p$, and $q$ are all provable primes

        \item[(ii)] $p_1$, $p_2$, $q_1$, $q_2$ are provable primes, and $p$ and $q$ are probable primes
        \item[(iii)] $p_1$, $p_2$, $q_1$, $q_2$, $p$, and $q$ are all probable primes
    \end{enumerate}
\end{enumerate}

For our implementation, we choose to generate probable primes $p$ and $q$ with conditions based on auxiliary probable primes $p_1$, $p_2$, $q_1$, and $q_2$. In other words, we choose the method (iii) listed above. While this method offers the lowest quality of primes, it offers the best performance. It would be interesting future work to benchmark key generation times and quality of primes among these three methods.

Method (iii) supports key sizes of length 1024, 2048, and 3072, which offers more utility over method (i), which offers only key sizes of length 2048 and 3072. For different key sizes, various lengths of auxiliary primes must be satisfied, which is reproduced in Table~\ref{tab:aux_len}. Table~\ref{tab:aux_len} can be joined with Table~\ref{tab:mr} for a comprehensive view of parameters as a function of the key size $nlen$.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|} 
 \hline
 Key Size $(nlen)$ & Minimum Length of Auxiliary Primes\\ \hline
 1024 bits & $> 100$ bits \\ 
 2048 bits & $> 140$ bits \\ 
 3072 bits & $> 170$ bits \\
 \hline
\end{tabular}
\caption{The table shows the minimum length of auxiliary primes $p_1$, $p_2$, $q_1$, and $q_2$ as a function of the key size $nlen$.}
\label{tab:aux_len}
\end{table}

Regarding our actual implementation of method (iii), we closely follow the constraints above and how probable primes are generated from probable auxiliary primes as specified in the DSS~\cite{fips2013186}. There are further constraints to the above, which are specific to method (iii), that we satisfy but do not fully detail here. However, one important aspect of method (iii) is that it leverages the Chinese Remainder Theorem to improve performance for key generation.\newline

\noindent \textbf{Private exponent $d$.} The following constraints must hold true for the private exponent $d$.

\begin{enumerate}
    \item The private exponent $d$ must be a positive integer between

        \begin{equation}
            2^{nlen/2} < d < LCM(p-1,q-1).
        \end{equation}

    \item $1 \equiv (ed) \pmod{LCM(p-1,q-1)}$.
\end{enumerate}

Implementing constraints for the private exponent $d$ is relatively straightforward. However, we do consider that in the rare case when $d \leq 2^{nlen/2}$, new primes must be generated.

\subsection{Encryption and Decryption}

We follow the PKCS $\#1$ v2.2 RSA Cryptography Standard~\cite{rsa2012pkcs} developed by RSA Laboratories to implement the encryption and decryption functions.

\subsection{Data Primitives}

As part of the PKCS specification, we implement the two cryptographic primitives, I2OSP and OS2IP, from scratch. I2OSP converts a nonnegative integer $x$ into a zero-padded octet string of length $xLen$. OS2IP converts an octet string back to an integer.

\subsection{Cryptographic Primitives}

The two cryptographic primitives are RSAEP, which is the encryption primitive, and RSADP, which is the decryption primitive. We implement these as prescribed in the specification, adapting the GMP Library.

\subsection{RSAES-OAEP}

RSAES-OAEP combines both of the cryptographic primitives aforementioned, and uses an encoding method based on Bellare and Rogaway's Optimal Assymetric Encryption Scheme~\cite{bellare1994optimal}. RSAES-OAEP is parameterized by a hash function and mask generation function that we describe further in Section~\cite{sec:hash}. Both the RSAES-OAEP-Encryption and RSAES-OAEP-Decryption operations are implemented as prescribed in the PKCS specification.


\subsection{Hash and Mask Generation}\label{sec:hash}

While there are numerous acceptable hash functions, SHA-512 and SHA-256 are recommended.
Thus, erring for performance, we choose to use the OpenSSL SHA-256 implementation. The Mask Generation Function (MGF) is crucial for the security of the RSA encryption scheme as specified. 
The MGF takes an octet string of varying length, and then outputs a pseudorandom octet string of a desired length. This means that the output cannot be predicted, and the provable securtiy of RSAES-OAEP relies on the MGF's randomness.

\section{Crypto Learning}

Here, we overview a number of strengths and weaknesses of our RSA implementation. In particular, we discuss attacks that we do protect against, and attacks that would cause our implementation to fail.

\subsection{Attacks via Insecure PRNGs}

We generate pseudorandom numbers using the {\tt /dev/random} file, as specified in Section~\ref{sec:prng}. This is considered a cryptographically secure method for generating pseudorandom numbers and is widely used in practice. Even so, there exist several theoretical attacks on Linux's implementation of this PRNG.

Gutterman \emph{et al.} perform an analysis of Linux's pseudorandom number generator (LRNG) and expose a number of security vulnerabilities~\cite{gutterman2006analysis}. More specifically, they reverse engineer LRNG and show that given the current state of the generator, it is possible to reconstruct previous states, thereby compromising the security of past usage. Further, they show that it is possible to measure and analyze the entropy created by the kernel. Bernstein presents a related
attack in which monitoring one source of entropy could compromise the randomness of other sources of entropy~\cite{bernstein2014entropy}.

While the latter attacks are theoretical, and to our knowledge have not been successful in practice, Gutterman also presents a denial of service attack that our implementation is susceptible to~\cite{gutterman2006analysis}. Since Linux's implementation of {\tt /dev/random} may block the output of bits when the entropy is low, one simple attack would be to simply read all the bits from {\tt /dev/random}, thereby blocking other users' access to new bits for a long
period of time. More interestingly, an attack can also be performed remotely by triggering system requests for {\tt get\_random\_bytes}, which will block both {\tt /dev/random} and the non-blocking {\tt /dev/urandom} pool.

One possible solution is to limit the per user consumption of random bits. Alternatively, we could avoid using {\tt /dev/random} altogether and instead generate pseudorandom numbers via hardware random number generators.

\subsection{Common Modulus Attack}
While the common modulus attack is simple, it is a case in point for the dangers of misusing RSA~\cite{boneh1999twenty}.

In order to prevent having to generate a different modulus $n$ for different users, a developer might choose to fix $n$ for a number of users or for all users. This is insecure, since a user could use his/her own exponents $e$ and $d$ to factor the fixed $n$, thereby recovering the private key $d$ from some other user. Thus, the common modulus attack shows that the RSA modulus should not be fixed. Our implementation precludes this attack by generating a random modulus every time.
This is done through calls to the {\tt gen\_primes} function.

\subsection{Low Private Exponent Attack}
In order to reduce the decryption time, a developer might choose a smaller value for the private exponent $d$ rather than a random value. Choosing a small $d$ can improve decryption performance (modular exponentation) by a factor of at least 10 for a 1024-bit modulus. However, Weiner shows that such a simplification is completely insecure~\cite{wiener1990cryptanalysis}. Boneh and Durfee further improve the bounds of Weiner's attack, showing that $d <
n^{0.292}$ is susceptible to attack~\cite{boneh2000new}. There are two techniques to prevent this attack; both of which our implementation supports. 

The first technique is to use a large public exponent $e$. Weiner shows that as long as $e > n^{1.5}$, this attack cannot be performed. In our implementation, we fix $e = 65537$. Thus, for $nlen = 1024$, our implementation supports this technique. However, this technique does not hold true for $nlen = 2048$ or $nlen = 3072$. This can be easily fixed by increasing $e$ to satisfy $nlen = 3072$, however, the downside is that it will increase encryption time. Nonetheless, the second technique, using
the Chinese Remainder Theorem to speed up decryption, is fully supported by our implementation.

\subsection{Low Public Exponent Attack}

Similar to the latter attack, in order to reduce the encryption time, a developer might choose a smaller value for the public exponent $e$. This engenders a number of attacks on low public exponents, most of which are based on Coppersmith's theorem~\cite{coppersmith1997small}. While the smallest $e$ possible is 3, $e \geq 2^{16} + 1$ is recommended to prevent certain attacks. This is the value of $e$ that we use in our implementation. It is simple to increase $e$ for security, but this will result in a performance decline.

\subsection{Partial Key Exposure Attack}

Suppose that for a given private key $(n,d)$, some portion of the private exponent $d$ is exposed. Boneh \emph{et al.} show that recovering the rest of the private exponent $d$ is possible when the corresponding private exponent $e$ is small. Specifically, they show that it is possible to reconstruct all of $d$ as long as $e < \sqrt{n}$. In our implementation, $e = 65537$ and all $nlen$ are secure from such an attack. However, partial key exposure attacks do illustrate the importance
of keeping the entire private key secret. This is one consideration that our implementation is lacking, and it will be interesting to explore this in the future.

\subsection{Side-Channel Attacks}

Kocher's seminal cryptanalysis of RSA via a timing attack shows that a clever attacker could measure the amount of time it takes for RSA decryption, thereby recovering the private exponent $d$~\cite{kocher1996timing}. Our implementation does not protect against such timing attacks, but there are two solutions that can be considered.

The first is to introduce a delay so that decryption (modular exponentiation, in particular) takes a fixed amount of time. However, this would cause a decline in performance. The second solution is based on blinding, by which a randomization is introduced such that decryption is performed on a random message unknown to the attacker. Thus, such timing attacks cannot be performed.

Kocher also discovered another side-channel attack by measuring the amount of power consumed during decryption. Since multiprecision multiplication causes greater power consumption, it is simple to detect the number of multiplications, thereby revealing information about the private exponent $d$. 

Another security concern we encountered when implementing the BigInteger library was a potential timing attack when performing modular exponentiation. The larger the encryption exponent, the long a na\"{i}ve implementation will take, and this leaks information about the length of the encryption exponent if an attacker can observe the time it takes to perform the exponentiation. For exponentiation using Montgomery multiplication, there is a modification called Montgomeryâ€™s Ladder which is used to make exponentiation run in constant time. We did not implement this modification, but it would be interesting to do so in the future.
\section{Secure Coding}

We next overview secure coding practices that we considered for our implementation, as well as practices that could have further improved our code. These are mostly based on the SEI CERT C Coding Standard~\cite{seacord2008cert}.

\subsection{Integers and Floats}

Handling multiple precision integers and multiple precision floats and understanding conversions between these data types is crucial in implementing RSA.

In regards to integers, we use different types of integers (i.e. {\tt int}, {\t unsigned long int}, and {\tt mpz\_t} (multiple precision integers) for different purposes. For general purpose counters, we can safely use the {\tt int} data type. For representing the size of an object, we can safely use the {\tt size\_t} data type, since this generally covers the entire address space. For any integers that may be used in multiple precision arithmetic, we err on the side of caution
and use the {\tt unsigned long int} data type. Then finally, for any integers that require multiple precision, we use the {\tt mpz\_t} data type from the GMP Library.

In regards to floating point numbers, we simply use the {\tt mpf\_t} data type from the GMP Library, since their use is limited and the multiple precision float data type offers enough utility for the required use cases.

Further, we also perform adequate range checking, integer overflow checking, and truncation checking. For the generation of key parameters, it is crucial that we perform range checking thoroughly, since a
single misstep could lead to an incorrect encryption or decryption. Additionally, we err on the side of caution and instantiate integers as either {\tt long int} or {\tt mpz\_t} to prevent integer overflows. Finally, we pay attention to any truncation that may occur as a result of conversions between integers and floats. For example, it is important to consider that while a multiple precision integer square root function is available, the result is
truncated to an integer. Thus, we must handle such operations more precisely using the {\tt mpf\_t} (multiple precision float) object.

\subsection{Memory Management}

Since memory owned by our process can be accessed and reused by another process in the absence of proper memory management, this could potentially reveal information about secret keys to other processes. Even further, systems with multiple users make it possible for one user to sniff keys from another users' process. Thus, proper memory management is crucial for the secrecy of private keys.

In this regard, we free dynamically allocated memory whenever it is no longer needed. This occurs throughout our implementation in two fashions. First, consider when a new block of memory is allocated using {\tt malloc}. Once the allocated block of memory is no longer in use, memory is freed using the function call {\tt free}. Second, when using the GMP Library to instantiate multiple precision numbers, these numbers are also dynamically allocated. Thus, this memory must
either be freed using the function call {\tt mpz\_clear} (for integers) or ``zeroized'' to ensure that no information about the secret keys are revealed.

The dynamic sizing ultimately proved to be very cumbersome to work with. For most operations, it wasn't possible to predict the number of bytes of storage that would be needed until after the result was computed. This resulted in excessive memory management (for example, reallocating memory after the operation to fit the size of the result) and significant performance overhead. It would have been better to assign a maximum size for a multi-precision integer, allocate a fixed block of that size, and let it grow or shrink as needed. Although this is a less efficient use of memory, the lack of overhead for managing memory would have cleaned up the code and increase performance significantly.

Likewise, avoiding in-place operations proved to be an inconvenience. On several occasions, it would have been more convenient to write back the result of an operation to one of the operands (for example, to use immediately afterword). But our library did not support this, so we were forced to allocate a new integer whether or not it wasn't necessary. This again resulted in unnecessary overhead due to memory management, and made the library more difficult to use.

\subsection{Characters and Strings}

One secure coding practice that we should have considered is to cast characters to {\tt unsigned char} before converting them to larger integer sizes. One instance of this is when generating pseudorandom numbers from {\tt /dev/random}, since we sample random characters from this file and then convert it to a pseudorandom multiple precision integer. More broadly, any arguments to character-handling functions should be represented as an {\tt unsigned char}.
However, this is only applicable to platforms in which {\tt char} data types have the same representation as {\tt signed char} data types.

\subsection{Error Handling}

Another secure coding practice that we should have considered is to handle errors throughout the entire program. Although there are instances in which we do handle errors, our program would be much more robust if it detected and handled all standard library errors and GMP Library errors. Having a consistent and comprehensive error-handling policy would improve our implementation's resilience in the face of erroneous or malicious inputs, hardware or software faults, and
unexpected environment changes. This would be advantageous both to the developers as well as the end-users of our implementation.

\subsection{Test Suite}

It would have been beneficial to set up a comprehensive test suite, which could rigorously test the modules within our implementation. Alternatively, we could have used a fuzzer to exercise the logic of our implementation. In the future, we can leverage static analysis techniques and a binary fuzzer, such as American Fuzzy Lop (AFL), to discover any bugs or vulnerabilities in our code.

\section{Summary}

Taken as a whole, this project illuminated many of the intricacies involved in a real-world implementation of the RSA cryptosystem, and cryptosystems more broadly. Truly, what we learn as ``textbook'' RSA is a tremendous oversimplification to what RSA is in practice. As expected, the learning outcomes from this project were innumerable as we were confronted with both number theoretic attacks, as well as implementation attacks. In regards to secure coding practices, perhaps the most
important learning outcome was realizing that the vast space of considerations makes cryptographic coding especially difficult

\bibliography{proposal}
\bibliographystyle{unsrt}

\appendix

\section{Code}

\lstinputlisting[language=C, numbers=left, breaklines=true, basicstyle=\ttfamily\footnotesize, captionpos=t, caption={Code for {\tt rsa.h}.}]{../src/rsa.h}

\lstinputlisting[language=C, numbers=left, breaklines=true, basicstyle=\ttfamily\footnotesize, captionpos=t, caption={Code for {\tt rsa.c}.}]{../src/rsa.c}

\lstinputlisting[language=C, numbers=left, breaklines=true, basicstyle=\ttfamily\footnotesize, captionpos=t, caption={Code for {\tt test.c}.}]{../src/test.c}


\section{Crypto Coding Practices}

\begin{enumerate}
    \item We learned how to use a cryptographically secure pseudorandom number generator and that even this has inherent disadvantages.

    \item We learned how to prevent the elementary common modulus attack.

    \item We learned how to prevent low private exponent attacks and that the security of our implementation can be improved further by choosing a larger public exponent.

    \item We learned how to prevent low public exponent attacks and that the security of our implementation can be improved further by choosing a larger public exponent.

    \item We learned how to prevent partial key exposure attacks and that our implementation is lacking in privacy provisions for the private exponent $d$.

    \item We learned that our implementation is not immune to timing attacks and power consumption attacks and that precluding these attacks is difficult.
\end{enumerate}

\section{Secure Coding Practices}

\begin{enumerate}
    \item We learned the importance of freeing dynamically allocated memory, especially in a cryptographic setting where unfreed memory can contain sensitive data.
        
    \item We learned to correctly size memory allocation for an object; using GMP Library for most instances greatly reduces developer errors.

    \item We learned the importance of converting {\tt char} data types to {\tt unsigned char} data types whenever it is being passed to a character-handling function.

    \item We learned retrospectively that consistent and comphrehensive error handling would have made the development effort much easier.

    \item We would have liked to create a comprehensive test suite that could ensure correctness through future iterations of our implementation. This would have been tremendously helpful during the development process.

    \item It would be interesting to leverage static analysis techniques and a binary fuzzer, such as AFL, to discover any unintended behavior in our implementation.
\end{enumerate}
\end{document}


